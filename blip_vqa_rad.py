# -*- coding: utf-8 -*-
"""BLIP-VQA RAD.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1am6VOezJ7OhG8ZR33024K_U53DUU9jZG
"""

# Setup necessary environments and libraries
!pip install ipywidgets transformers[torch] datasets ipywidgets \
    torch torchvision torchaudio \
    --index-url https://download.pytorch.org/whl/cu118

import torch
import numpy as np
import requests
import matplotlib.pyplot as plt
from PIL import Image
from tqdm.notebook import tqdm
from datasets import load_dataset
from torch.utils.data import DataLoader
from IPython.display import display
from transformers import (
    BlipProcessor,
    BlipForQuestionAnswering,
    BlipImageProcessor,
    AutoProcessor,
    BlipConfig
)

# Assign the computation engine (GPU preference)
compute_device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# Load the VQA-RAD medical dataset
vqa_rad_data = load_dataset("flaviagiammarino/vqa-rad")

vqa_rad_data

# Displaying a sample from the training set with its question and answer
sample_data = vqa_rad_data['train'][1]
plt.imshow(sample_data['image'].convert('RGB'))
print(f"Question: {sample_data['question']}")
print(f"Answer: {sample_data['answer']}")

# Load the default BLIP configuration
vqa_config = BlipConfig.from_pretrained("Salesforce/blip-vqa-base")

# Create data iterators
train_iter = DataLoader(vqa_rad_data["train"], shuffle=True, batch_size=4)
test_iter = DataLoader(vqa_rad_data["test"], batch_size=4)

# Load the pre-trained BLIP model
blip_vqa_model = BlipForQuestionAnswering.from_pretrained("Salesforce/blip-vqa-base")
blip_vqa_model.to(compute_device)

# Initialize optimization algorithm
vqa_optimizer = torch.optim.AdamW(blip_vqa_model.parameters(), lr=5e-5)

from torchvision import transforms

class MedVQADataset(torch.utils.data.Dataset):
    def __init__(self, dataset_split, text_proc, img_proc, transform=None): # Added transform
        self.dataset = dataset_split
        self.queries = dataset_split['question']
        self.responses = dataset_split['answer']
        self.text_proc = text_proc
        self.img_proc = img_proc
        self.limit_len = 32
        self.target_size = 128
        self.transform = transform # Store transform

    def __len__(self):
        return len(self.dataset)

    def __getitem__(self, idx):
        ans = self.responses[idx]
        ques = self.queries[idx]
        img = self.dataset[idx]['image'].convert('RGB')

        # --- NEW: Apply transformation if provided ---
        if self.transform:
            img = self.transform(img)

        # Image processing with fixed dimensions
        img_encoding = self.img_proc(img, do_resize=True, size=(self.target_size, self.target_size), return_tensors="pt")
        text_encoding = self.text_proc(None, ques, padding="max_length", truncation=True, max_length=self.limit_len, return_tensors="pt")

        item = {k: v.squeeze() for k, v in text_encoding.items()}
        item["pixel_values"] = img_encoding["pixel_values"][0]

        item["labels"] = self.text_proc.tokenizer.encode(
            ans, max_length=self.limit_len, padding="max_length", truncation=True, return_tensors='pt'
        )[0]

        return item

# Setup the dual-modality processors
caption_processor = BlipProcessor.from_pretrained("Salesforce/blip-vqa-base")
visual_processor = BlipImageProcessor.from_pretrained("Salesforce/blip-vqa-base")

from torchvision import transforms

# Define augmentations
train_transforms = transforms.Compose([
    transforms.RandomRotation(degrees=10),
    transforms.RandomHorizontalFlip(p=0.5),
    transforms.ColorJitter(brightness=0.1, contrast=0.1)
])

# Create objects: Augmentation only on train_vqa
train_vqa = MedVQADataset(vqa_rad_data['train'], caption_processor, visual_processor, transform=train_transforms)
test_vqa = MedVQADataset(vqa_rad_data['test'], caption_processor, visual_processor, transform=None)

train_vqa[0]

def vqa_collate(batch_list):
    # Stacking individual components into batches
    return {
        'input_ids': torch.stack([x['input_ids'] for x in batch_list]),
        'attention_mask': torch.stack([x['attention_mask'] for x in batch_list]),
        'pixel_values': torch.stack([x['pixel_values'] for x in batch_list]),
        'labels': torch.stack([x['labels'] for x in batch_list])
    }

train_loader = DataLoader(train_vqa, collate_fn=vqa_collate, batch_size=64, shuffle=False)
test_loader = DataLoader(test_vqa, collate_fn=vqa_collate, batch_size=64, shuffle=False)

# Check shapes of the first batch
sample_batch = next(iter(train_loader))
for key, value in sample_batch.items():
    print(f"{key}: {value.shape}")

from transformers import AutoProcessor, BlipForQuestionAnswering

# Model initialization
blip_model = BlipForQuestionAnswering.from_pretrained("Salesforce/blip-vqa-base")
blip_model.to(compute_device)
main_processor = AutoProcessor.from_pretrained("Salesforce/blip-vqa-base")

# Setup AdamW optimizer with original learning rate
vqa_optimizer = torch.optim.AdamW(blip_model.parameters(), lr=5e-5)

# Saving mean/std for later image restoration
normalization_mean = visual_processor.image_mean
normalization_std = visual_processor.image_std

# Visualizing an item from the batch to ensure correct processing
idx_to_view = 1
img_array = (sample_batch["pixel_values"][idx_to_view].cpu().numpy() * np.array(normalization_std)[:, None, None]) + np.array(normalization_mean)[:, None, None]
img_array = np.moveaxis(img_array, 0, -1)
img_array = (img_array * 255).astype(np.uint8)

print("Q:", caption_processor.decode(sample_batch["input_ids"][idx_to_view]))
print("A:", caption_processor.decode(sample_batch["labels"][idx_to_view]))
plt.imshow(Image.fromarray(img_array))

import copy

# --- TRACKER VARIABLES ---
train_losses, train_accuracies = [], []
test_losses, test_accuracies = [], []

best_val_loss = float('inf')
best_model_state = None

# Early Stopping parameters
early_stop_patience = 3
epochs_without_improvement = 0

epochs = 15

for epoch in range(epochs):
    # --- TRAINING PHASE ---
    blip_model.train()
    epoch_loss = 0

    progress = tqdm(train_loader, desc=f"Epoch {epoch+1}/{epochs}")
    for batch in progress:
        vqa_optimizer.zero_grad()
        batch = {k: v.to(compute_device) for k, v in batch.items()}
        outputs = blip_model(**batch)

        loss = outputs.loss
        loss.backward()
        vqa_optimizer.step()
        epoch_loss += loss.item()

    # --- EVALUATION PHASE ---
    # Calculate metrics for plotting
    current_train_loss, current_train_acc = run_evaluation(blip_model, train_loader)
    current_test_loss, current_test_acc = run_evaluation(blip_model, test_loader)

    # Append to history
    train_losses.append(current_train_loss); train_accuracies.append(current_train_acc)
    test_losses.append(current_test_loss); test_accuracies.append(current_test_acc)

    print(f"Epoch {epoch+1}: [Train] Loss: {current_train_loss:.4f}, Acc: {current_train_acc:.2f}% | [Test] Loss: {current_test_loss:.4f}, Acc: {current_test_acc:.2f}%")

    # --- BEST MODEL TRACKING & EARLY STOPPING ---
    if current_test_loss < best_val_loss:
        best_val_loss = current_test_loss
        # Store state in CPU memory to avoid filling up GPU
        best_model_state = {k: v.cpu().clone() for k, v in blip_model.state_dict().items()}
        epochs_without_improvement = 0  # Reset counter
        print(f"--> New best model found! Saved state at Epoch {epoch+1}")
    else:
        epochs_without_improvement += 1
        print(f"No improvement for {epochs_without_improvement} epoch(s).")

    # Trigger Early Stopping
    if epochs_without_improvement >= early_stop_patience:
        print(f"\nEarly stopping triggered after {epoch+1} epochs.")
        # Restore the best state back to the model
        blip_model.load_state_dict(best_model_state)
        break

import matplotlib.pyplot as plt

# Create a figure with two subplots
plt.figure(figsize=(14, 5))

# Plot 1: Loss History
plt.subplot(1, 2, 1)
plt.plot(range(1, len(train_losses) + 1), train_losses, label='Train Loss', marker='o', color='#1f77b4')
plt.plot(range(1, len(test_losses) + 1), test_losses, label='Test Loss', marker='o', color='#ff7f0e')
plt.title('Training and Test Loss', fontsize=14)
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.grid(True, linestyle='--', alpha=0.7)

# Plot 2: Accuracy History
plt.subplot(1, 2, 2)
plt.plot(range(1, len(train_accuracies) + 1), train_accuracies, label='Train Acc', marker='o', color='#1f77b4')
plt.plot(range(1, len(test_accuracies) + 1), test_accuracies, label='Test Acc', marker='o', color='#ff7f0e')
plt.title('Training and Test Accuracy', fontsize=14)
plt.xlabel('Epochs')
plt.ylabel('Accuracy (%)')
plt.legend()
plt.grid(True, linestyle='--', alpha=0.7)

plt.tight_layout()
plt.show()

import os

# 1. Check if the best state actually exists in memory
if 'best_model_state' in globals() and best_model_state is not None:
    # 2. Define the local folder name
    checkpoint_folder = "./blip_vqa_rad_best_model"

    # 3. Load the best weights back into the model variable
    blip_model.load_state_dict(best_model_state)

    # 4. Save the model and processor to the local folder
    blip_model.save_pretrained(checkpoint_folder)
    main_processor.save_pretrained(checkpoint_folder)

    print(f" Best weights from memory have been saved to: {checkpoint_folder}")
else:
    print(" Error: 'best_model_state' not found in memory. You must have run the training loop at least once in this session.")

from google.colab import drive
import shutil

# 1. Mount Drive
drive.mount('/content/drive')

# 2. Define paths
local_source = "./blip_vqa_rad_best_model"
drive_destination = "/content/drive/MyDrive/VQA_Models/blip_vqa_rad_model"

# 3. Copy to Drive
try:
    # Create the VQA_Models folder if it doesn't exist
    os.makedirs(os.path.dirname(drive_destination), exist_ok=True)

    # Remove existing folder in Drive to prevent merging files
    if os.path.exists(drive_destination):
        shutil.rmtree(drive_destination)

    shutil.copytree(local_source, drive_destination)
    print(f" SUCCESS!  best model is now in Drive at: {drive_destination}")
except Exception as e:
    print(f" Export failed: {e}")

"""VALIDATION TESTING"""

!pip install evaluate rouge_score

import torch
import evaluate
from tqdm.notebook import tqdm
from transformers import BlipForQuestionAnswering, AutoProcessor

# 1. Define the GDrive path
gdrive_path = "/content/drive/MyDrive/VQA_Models/blip_vqa_rad_model"

print(f" Importing model from GDrive: {gdrive_path}")
# Load the model and processor
model = BlipForQuestionAnswering.from_pretrained(gdrive_path).to(compute_device)
processor = AutoProcessor.from_pretrained(gdrive_path)
model.eval()

# 2. Load Metric Tools
bleu = evaluate.load("bleu")
rouge = evaluate.load("rouge")

# 3. Evaluation Loop
results = {'closed': {'preds': [], 'labels': []}, 'open': {'preds': [], 'labels': []}}
closed_set = {'yes', 'no'}

with torch.no_grad():
    for batch in tqdm(test_loader, desc="Evaluating Test Set"):
        batch = {k: v.to(compute_device) for k, v in batch.items()}

        # Generate predictions
        generated_ids = model.generate(
            pixel_values=batch['pixel_values'],
            input_ids=batch['input_ids'],
            attention_mask=batch['attention_mask'],
            max_length=32
        )

        preds = processor.batch_decode(generated_ids, skip_special_tokens=True)
        labels = processor.batch_decode(batch['labels'], skip_special_tokens=True)

        for p, l in zip(preds, labels):
            p_clean = p.strip().lower()
            l_clean = l.strip().lower()

            # Categorize based on the ground truth answer
            if l_clean in closed_set:
                results['closed']['preds'].append(p_clean)
                results['closed']['labels'].append(l_clean)
            else:
                results['open']['preds'].append(p_clean)
                results['open']['labels'].append(l_clean)

# 4. Calculate and Print Metrics
print("\n" + "="*60)
print(f"{'DETAILED EVALUATION RESULTS':^60}")
print("="*60)

# --- CLOSED ENDED ---
c_preds = results['closed']['preds']
c_labels = results['closed']['labels']
if c_labels:
    c_acc = (sum(1 for p, l in zip(c_preds, c_labels) if p == l) / len(c_labels)) * 100
    print(f"CLOSED-ENDED QUESTIONS (n={len(c_labels)})")
    print(f" - Accuracy: {c_acc:.2f}%")
else:
    print("No closed-ended questions found in test set.")

print("-" * 60)

# OPEN ENDED
o_preds = results['open']['preds']
o_labels = results['open']['labels']
if o_labels:
    # Open-ended Accuracy (Exact Match)
    o_acc = (sum(1 for p, l in zip(o_preds, o_labels) if p == l) / len(o_labels)) * 100

    # BLEU Score (requires list of references for each prediction)
    bleu_score = bleu.compute(predictions=o_preds, references=[[l] for l in o_labels])['bleu'] * 100

    # ROUGE-L Score
    rouge_score = rouge.compute(predictions=o_preds, references=o_labels)['rougeL'] * 100

    print(f"OPEN-ENDED QUESTIONS (n={len(o_labels)})")
    print(f" - Accuracy: {o_acc:.2f}%")
    print(f" - BLEU:     {bleu_score:.2f}")
    print(f" - ROUGE-L:  {rouge_score:.2f}")
else:
    print("No open-ended questions found in test set.")

print("="*60)

import random
import numpy as np
import torch
from PIL import Image
from transformers import BlipForQuestionAnswering, AutoProcessor

# 1. Setup GDrive Path and Load Model

gdrive_path = "/content/drive/MyDrive/VQA_Models/blip_vqa_rad_model"

print(f"Loading model from Drive: {gdrive_path}")
model = BlipForQuestionAnswering.from_pretrained(gdrive_path).to(compute_device)
processor = AutoProcessor.from_pretrained(gdrive_path)
model.eval()

# 2. Select 30 random samples from the test set
num_samples = 30

random_indices = random.sample(range(len(test_vqa)), num_samples)

# 3. Loop through samples and generate answers
for idx in random_indices:
    current_sample = test_vqa[idx]

    print("-" * 50)
    # Decode and print Question
    question_text = processor.decode(current_sample['input_ids'], skip_special_tokens=True)
    print(f"Question: {question_text}")

    # Prepare inputs for the model (add batch dimension and move to GPU)
    inference_batch = {k: v.unsqueeze(0).to(compute_device) for k, v in current_sample.items()}

    # Model Generation
    with torch.no_grad():
        generated_ids = model.generate(
            pixel_values=inference_batch['pixel_values'],
            input_ids=inference_batch['input_ids'],
            max_length=32
        )

    # Decode predictions and ground truth
    prediction = processor.decode(generated_ids[0], skip_special_tokens=True)
    actual = processor.decode(current_sample['labels'], skip_special_tokens=True)

    print(f"Predicted Answer: {prediction}")
    print(f"Actual Answer:    {actual}")

    # 4. Denormalize Image for display

    img_tensor = inference_batch["pixel_values"][0].cpu().numpy()
    mean = np.array(normalization_mean)[:, None, None]
    std = np.array(normalization_std)[:, None, None]

    # Reverse normalization: (tensor * std) + mean
    visual_array = (img_tensor * std) + mean
    visual_array = (visual_array.clip(0, 1) * 255).astype(np.uint8)
    visual_array = np.moveaxis(visual_array, 0, -1)

    # Display the result
    display(Image.fromarray(visual_array))